{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f615b5b5-8017-43e5-b681-ed5d8713630b",
   "metadata": {},
   "source": [
    "## Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e122d5-7941-442d-92fe-843d0d639adb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "To solve this problem, we need to use Bayes' theorem, which relates conditional probabilities. Let's define:\n",
    "\n",
    "A: an employee uses the company's health insurance plan\n",
    "B: an employee is a smoker\n",
    "\n",
    "We want to find the probability of an employee being a smoker given that he/she uses the health insurance plan, which is P(B|A).\n",
    "\n",
    "We know that 70% of the employees use the health insurance plan, which means P(A) = 0.7.\n",
    "\n",
    "We also know that 40% of the employees who use the plan are smokers, which means P(B|A) = 0.4.\n",
    "\n",
    "Bayes' theorem states that: P(B|A) = P(A|B) * P(B) / P(A)\n",
    "\n",
    "We need to find P(B), which is the probability of an employee being a smoker regardless of whether they use the health insurance plan or not. We can use the law of total probability to calculate it:\n",
    "\n",
    "P(B) = P(B|A) * P(A) + P(B|A') * P(A')\n",
    "\n",
    "where A' means an employee does not use the health insurance plan. We can assume that the percentage of non-users of the plan who are smokers is negligible, so P(B|A') ≈ 0. Therefore:\n",
    "\n",
    "P(B) ≈ P(B|A) * P(A) + 0\n",
    "\n",
    "P(B) ≈ 0.4 * 0.7 = 0.28\n",
    "\n",
    "Now we can plug in all the values into Bayes' theorem:\n",
    "\n",
    "P(B|A) = P(A|B) * P(B) / P(A)\n",
    "\n",
    "P(B|A) = P(A and B) / P(A)\n",
    "\n",
    "P(B|A) = P(B|A) * P(A) / P(A)\n",
    "\n",
    "P(B|A) = 0.4 * 0.7 / 0.7\n",
    "\n",
    "P(B|A) = 0.4\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.4 or 40%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efee751-6182-491f-90a3-1ebb14f7af0f",
   "metadata": {},
   "source": [
    "## Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c80cec9-ff2f-4beb-8dba-d78eef99a59d",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm used for classification tasks, especially in natural language processing and text classification. They are both based on Bayes' theorem and the assumption of conditional independence among features given the class label. However, they differ in the type of data they are designed to handle and their underlying probability models.\n",
    "\n",
    "### Bernoulli Naive Bayes:\n",
    "\n",
    "- Suitable for binary feature data, where each feature can take only two values (usually 0 and 1).\n",
    "- The presence or absence of a feature in a document is represented by binary values (0 or 1).\n",
    "- Assumes that each feature is conditionally independent given the class label.\n",
    "- Works well for tasks like document classification, spam filtering, sentiment analysis, etc., where the presence or absence of certain words or features in a document is essential.\n",
    "\n",
    "### Multinomial Naive Bayes:\n",
    "\n",
    "- Suited for discrete count data, commonly used for text classification tasks, where features represent word counts or frequency.\n",
    "- Instead of binary values, it deals with integer counts of occurrences of each feature (word) in a document.\n",
    "- Assumes that each feature's count (word frequency) is conditionally independent given the class label.\n",
    "- It is particularly useful when we want to take into account the frequency or distribution of words in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d9123f-a57b-4439-9940-50b780aa1b96",
   "metadata": {},
   "source": [
    "## Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e50f6-ee95-4f0e-a5ac-3e09cc4d3e55",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes handles missing values by ignoring them during the calculation of probabilities. When encountering a missing value (a feature with no information) for a particular instance during classification, Bernoulli Naive Bayes simply excludes that feature from the calculation of probabilities for that instance.\n",
    "\n",
    "To understand this better, let's recap the fundamental steps of the Bernoulli Naive Bayes algorithm:\n",
    "\n",
    "### Training Phase:\n",
    "\n",
    "- Bernoulli Naive Bayes estimates probabilities for each feature (binary-valued) in each class based on the training data. It calculates the probabilities of a feature being 1 (present) and 0 (absent) for each class.\n",
    "\n",
    "### Classification Phase:\n",
    "\n",
    "- When classifying a new instance (document), Bernoulli Naive Bayes calculates the conditional probability of each class given the presence or absence of each feature (word) in the instance.\n",
    "- If a feature is missing (not available) in the instance, it is simply ignored during the probability calculation.\n",
    "\n",
    "Since missing values are ignored, they do not influence the probability estimates for the class labels. In practice, this can be beneficial, especially when working with sparse data (data with many missing values), as it avoids making predictions based on limited or uncertain information.\n",
    "\n",
    "It's worth noting that the Naive Bayes algorithm, including Bernoulli Naive Bayes, is generally robust to missing values because of the conditional independence assumption. The absence of a particular feature (missing value) in an instance won't impact the classification decision much since the features are assumed to be independent of each other, given the class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c89bba-2b5c-4cd2-afc7-475c5dddcc20",
   "metadata": {},
   "source": [
    "## Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a1a1b8-1060-47fc-9b5a-36ce88879760",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that is designed to handle continuous (real-valued) features. It assumes that the features in each class follow a Gaussian (normal) distribution. While the original Naive Bayes algorithm can handle binary and count-based data (Bernoulli and Multinomial Naive Bayes, respectively), Gaussian Naive Bayes is appropriate for datasets with continuous numeric features.\n",
    "\n",
    "\n",
    "The multi-class classification is a scenario where an instance can belong to one of multiple classes. For example, classifying an image of an animal into categories like \"cat,\" \"dog,\" \"bird,\" or \"elephant\" is a multi-class classification task.\n",
    "\n",
    "\n",
    "In Gaussian Naive Bayes for multi-class classification, the algorithm estimates the parameters of the Gaussian distribution (mean and variance) for each continuous feature in each class during the training phase. Then, during the classification phase, it calculates the probability of an instance belonging to each class using the Gaussian probability density function based on the feature values of the instance and the estimated parameters for each class.\n",
    "\n",
    "The final decision for the class label is made based on the class with the highest probability for the given instance.\n",
    "\n",
    "Gaussian Naive Bayes is computationally efficient and easy to implement. However, it assumes that the features in each class follow a Gaussian distribution and that they are conditionally independent given the class label. These assumptions may not always hold in practice, especially for complex and high-dimensional data.\n",
    "\n",
    "While Gaussian Naive Bayes can be used for multi-class classification, it's essential to consider the characteristics of your data and the assumptions of the algorithm. Depending on the specific problem and dataset, other classifiers like Logistic Regression, Decision Trees, Random Forests, or Support Vector Machines may be more suitable for multi-class classification tasks. It's always a good idea to experiment with multiple algorithms and compare their performance to find the best approach for your specific problem.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c9ecc8-7a5d-4392-97ad-74c9b3b21684",
   "metadata": {},
   "source": [
    "## Q5. Assignment:\n",
    "### Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "#### Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "#### Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "#### Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b69b176-0983-45ef-a300-365f30cf92d1",
   "metadata": {},
   "source": [
    "Introduction: In this assignment, we will implement and compare the performance of three variants of Naive Bayes classifiers: Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes on the \"Spambase Data Set\" from the UCI Machine Learning Repository. We will use the scikit-learn library in Python for implementation and 10-fold cross-validation for evaluation.\n",
    "\n",
    "Data Preparation: First, we need to download the Spambase Data Set from the UCI Machine Learning Repository. The dataset contains 4601 email messages, where the goal is to predict whether a message is spam or not based on several input features. The features include the frequency of various words, characters, and punctuation marks, as well as information about the length of the message and the number of capital letters in the message.\n",
    "\n",
    "Implementation: We will now implement the three variants of Naive Bayes classifiers using the scikit-learn library in Python. The implementation is straightforward, and we will use the default hyperparameters for each classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "15cc73ec-cd49-4bdc-bac4-a0439dfdee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spambase.names','r') as f:\n",
    "    a = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e5d38ec-e88f-4bdd-bc29-17835b3b234f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| SPAM E-MAIL DATABASE ATTRIBUTES (in .names format)\n",
      "|\n",
      "| 48 continuous real [0,100] attributes of type word_freq_WORD \n",
      "| = percentage of words in the e-mail that match WORD,\n",
      "| i.e. 100 * (number of times the WORD appears in the e-mail) / \n",
      "| total number of words in e-mail.  A \"word\" in this case is any \n",
      "| string of alphanumeric characters bounded by non-alphanumeric \n",
      "| characters or end-of-string.\n",
      "|\n",
      "| 6 continuous real [0,100] attributes of type char_freq_CHAR\n",
      "| = percentage of characters in the e-mail that match CHAR,\n",
      "| i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n",
      "|\n",
      "| 1 continuous real [1,...] attribute of type capital_run_length_average\n",
      "| = average length of uninterrupted sequences of capital letters\n",
      "|\n",
      "| 1 continuous integer [1,...] attribute of type capital_run_length_longest\n",
      "| = length of longest uninterrupted sequence of capital letters\n",
      "|\n",
      "| 1 continuous integer [1,...] attribute of type capital_run_length_total\n",
      "| = sum of length of uninterrupted sequences of capital letters\n",
      "| = total number of capital letters in the e-mail\n",
      "|\n",
      "| 1 nominal {0,1} class attribute of type spam\n",
      "| = denotes whether the e-mail was considered spam (1) or not (0), \n",
      "| i.e. unsolicited commercial e-mail.  \n",
      "|\n",
      "| For more information, see file 'spambase.DOCUMENTATION' at the\n",
      "| UCI Machine Learning Repository: http://www.ics.uci.edu/~mlearn/MLRepository.html\n",
      "\n",
      "\n",
      "1, 0.    | spam, non-spam classes\n",
      "\n",
      "word_freq_make:         continuous.\n",
      "word_freq_address:      continuous.\n",
      "word_freq_all:          continuous.\n",
      "word_freq_3d:           continuous.\n",
      "word_freq_our:          continuous.\n",
      "word_freq_over:         continuous.\n",
      "word_freq_remove:       continuous.\n",
      "word_freq_internet:     continuous.\n",
      "word_freq_order:        continuous.\n",
      "word_freq_mail:         continuous.\n",
      "word_freq_receive:      continuous.\n",
      "word_freq_will:         continuous.\n",
      "word_freq_people:       continuous.\n",
      "word_freq_report:       continuous.\n",
      "word_freq_addresses:    continuous.\n",
      "word_freq_free:         continuous.\n",
      "word_freq_business:     continuous.\n",
      "word_freq_email:        continuous.\n",
      "word_freq_you:          continuous.\n",
      "word_freq_credit:       continuous.\n",
      "word_freq_your:         continuous.\n",
      "word_freq_font:         continuous.\n",
      "word_freq_000:          continuous.\n",
      "word_freq_money:        continuous.\n",
      "word_freq_hp:           continuous.\n",
      "word_freq_hpl:          continuous.\n",
      "word_freq_george:       continuous.\n",
      "word_freq_650:          continuous.\n",
      "word_freq_lab:          continuous.\n",
      "word_freq_labs:         continuous.\n",
      "word_freq_telnet:       continuous.\n",
      "word_freq_857:          continuous.\n",
      "word_freq_data:         continuous.\n",
      "word_freq_415:          continuous.\n",
      "word_freq_85:           continuous.\n",
      "word_freq_technology:   continuous.\n",
      "word_freq_1999:         continuous.\n",
      "word_freq_parts:        continuous.\n",
      "word_freq_pm:           continuous.\n",
      "word_freq_direct:       continuous.\n",
      "word_freq_cs:           continuous.\n",
      "word_freq_meeting:      continuous.\n",
      "word_freq_original:     continuous.\n",
      "word_freq_project:      continuous.\n",
      "word_freq_re:           continuous.\n",
      "word_freq_edu:          continuous.\n",
      "word_freq_table:        continuous.\n",
      "word_freq_conference:   continuous.\n",
      "char_freq_;:            continuous.\n",
      "char_freq_(:            continuous.\n",
      "char_freq_[:            continuous.\n",
      "char_freq_!:            continuous.\n",
      "char_freq_$:            continuous.\n",
      "char_freq_#:            continuous.\n",
      "capital_run_length_average: continuous.\n",
      "capital_run_length_longest: continuous.\n",
      "capital_run_length_total:   continuous.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d1ddee1-44c4-4196-87fd-98121de5bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spambase.DOCUMENTATION',mode='r') as f1:\n",
    "    b=f1.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "baffc525-729a-4474-9727-09d3aa5dffc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Title:  SPAM E-mail Database\n",
      "\n",
      "2. Sources:\n",
      "   (a) Creators: Mark Hopkins, Erik Reeber, George Forman, Jaap Suermondt\n",
      "        Hewlett-Packard Labs, 1501 Page Mill Rd., Palo Alto, CA 94304\n",
      "   (b) Donor: George Forman (gforman at nospam hpl.hp.com)  650-857-7835\n",
      "   (c) Generated: June-July 1999\n",
      "\n",
      "3. Past Usage:\n",
      "   (a) Hewlett-Packard Internal-only Technical Report. External forthcoming.\n",
      "   (b) Determine whether a given email is spam or not.\n",
      "   (c) ~7% misclassification error.\n",
      "       False positives (marking good mail as spam) are very undesirable.\n",
      "       If we insist on zero false positives in the training/testing set,\n",
      "       20-25% of the spam passed through the filter.\n",
      "\n",
      "4. Relevant Information:\n",
      "        The \"spam\" concept is diverse: advertisements for products/web\n",
      "        sites, make money fast schemes, chain letters, pornography...\n",
      "\tOur collection of spam e-mails came from our postmaster and \n",
      "\tindividuals who had filed spam.  Our collection of non-spam \n",
      "\te-mails came from filed work and personal e-mails, and hence\n",
      "\tthe word 'george' and the area code '650' are indicators of \n",
      "\tnon-spam.  These are useful when constructing a personalized \n",
      "\tspam filter.  One would either have to blind such non-spam \n",
      "\tindicators or get a very wide collection of non-spam to \n",
      "\tgenerate a general purpose spam filter.\n",
      "\n",
      "        For background on spam:\n",
      "        Cranor, Lorrie F., LaMacchia, Brian A.  Spam! \n",
      "        Communications of the ACM, 41(8):74-83, 1998.\n",
      "\n",
      "5. Number of Instances: 4601 (1813 Spam = 39.4%)\n",
      "\n",
      "6. Number of Attributes: 58 (57 continuous, 1 nominal class label)\n",
      "\n",
      "7. Attribute Information:\n",
      "The last column of 'spambase.data' denotes whether the e-mail was \n",
      "considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  \n",
      "Most of the attributes indicate whether a particular word or\n",
      "character was frequently occuring in the e-mail.  The run-length\n",
      "attributes (55-57) measure the length of sequences of consecutive \n",
      "capital letters.  For the statistical measures of each attribute, \n",
      "see the end of this file.  Here are the definitions of the attributes:\n",
      "\n",
      "48 continuous real [0,100] attributes of type word_freq_WORD \n",
      "= percentage of words in the e-mail that match WORD,\n",
      "i.e. 100 * (number of times the WORD appears in the e-mail) / \n",
      "total number of words in e-mail.  A \"word\" in this case is any \n",
      "string of alphanumeric characters bounded by non-alphanumeric \n",
      "characters or end-of-string.\n",
      "\n",
      "6 continuous real [0,100] attributes of type char_freq_CHAR\n",
      "= percentage of characters in the e-mail that match CHAR,\n",
      "i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n",
      "\n",
      "1 continuous real [1,...] attribute of type capital_run_length_average\n",
      "= average length of uninterrupted sequences of capital letters\n",
      "\n",
      "1 continuous integer [1,...] attribute of type capital_run_length_longest\n",
      "= length of longest uninterrupted sequence of capital letters\n",
      "\n",
      "1 continuous integer [1,...] attribute of type capital_run_length_total\n",
      "= sum of length of uninterrupted sequences of capital letters\n",
      "= total number of capital letters in the e-mail\n",
      "\n",
      "1 nominal {0,1} class attribute of type spam\n",
      "= denotes whether the e-mail was considered spam (1) or not (0), \n",
      "i.e. unsolicited commercial e-mail.  \n",
      "\n",
      "\n",
      "8. Missing Attribute Values: None\n",
      "\n",
      "9. Class Distribution:\n",
      "\tSpam\t  1813  (39.4%)\n",
      "\tNon-Spam  2788  (60.6%)\n",
      "\n",
      "\n",
      "Attribute Statistics:\n",
      "   Min: Max:   Average:  Std.Dev: Coeff.Var_%: \n",
      "1  0    4.54   0.10455   0.30536  292          \n",
      "2  0    14.28  0.21301   1.2906   606          \n",
      "3  0    5.1    0.28066   0.50414  180          \n",
      "4  0    42.81  0.065425  1.3952   2130         \n",
      "5  0    10     0.31222   0.67251  215          \n",
      "6  0    5.88   0.095901  0.27382  286          \n",
      "7  0    7.27   0.11421   0.39144  343          \n",
      "8  0    11.11  0.10529   0.40107  381          \n",
      "9  0    5.26   0.090067  0.27862  309          \n",
      "10 0    18.18  0.23941   0.64476  269          \n",
      "11 0    2.61   0.059824  0.20154  337          \n",
      "12 0    9.67   0.5417    0.8617   159          \n",
      "13 0    5.55   0.09393   0.30104  320          \n",
      "14 0    10     0.058626  0.33518  572          \n",
      "15 0    4.41   0.049205  0.25884  526          \n",
      "16 0    20     0.24885   0.82579  332          \n",
      "17 0    7.14   0.14259   0.44406  311          \n",
      "18 0    9.09   0.18474   0.53112  287          \n",
      "19 0    18.75  1.6621    1.7755   107          \n",
      "20 0    18.18  0.085577  0.50977  596          \n",
      "21 0    11.11  0.80976   1.2008   148          \n",
      "22 0    17.1   0.1212    1.0258   846          \n",
      "23 0    5.45   0.10165   0.35029  345          \n",
      "24 0    12.5   0.094269  0.44264  470          \n",
      "25 0    20.83  0.5495    1.6713   304          \n",
      "26 0    16.66  0.26538   0.88696  334          \n",
      "27 0    33.33  0.7673    3.3673   439          \n",
      "28 0    9.09   0.12484   0.53858  431          \n",
      "29 0    14.28  0.098915  0.59333  600          \n",
      "30 0    5.88   0.10285   0.45668  444          \n",
      "31 0    12.5   0.064753  0.40339  623          \n",
      "32 0    4.76   0.047048  0.32856  698          \n",
      "33 0    18.18  0.097229  0.55591  572          \n",
      "34 0    4.76   0.047835  0.32945  689          \n",
      "35 0    20     0.10541   0.53226  505          \n",
      "36 0    7.69   0.097477  0.40262  413          \n",
      "37 0    6.89   0.13695   0.42345  309          \n",
      "38 0    8.33   0.013201  0.22065  1670         \n",
      "39 0    11.11  0.078629  0.43467  553          \n",
      "40 0    4.76   0.064834  0.34992  540          \n",
      "41 0    7.14   0.043667  0.3612   827          \n",
      "42 0    14.28  0.13234   0.76682  579          \n",
      "43 0    3.57   0.046099  0.22381  486          \n",
      "44 0    20     0.079196  0.62198  785          \n",
      "45 0    21.42  0.30122   1.0117   336          \n",
      "46 0    22.05  0.17982   0.91112  507          \n",
      "47 0    2.17   0.0054445 0.076274 1400         \n",
      "48 0    10     0.031869  0.28573  897          \n",
      "49 0    4.385  0.038575  0.24347  631          \n",
      "50 0    9.752  0.13903   0.27036  194          \n",
      "51 0    4.081  0.016976  0.10939  644          \n",
      "52 0    32.478 0.26907   0.81567  303          \n",
      "53 0    6.003  0.075811  0.24588  324          \n",
      "54 0    19.829 0.044238  0.42934  971          \n",
      "55 1    1102.5 5.1915    31.729   611          \n",
      "56 1    9989   52.173    194.89   374          \n",
      "57 1    15841  283.29    606.35   214          \n",
      "58 0    1      0.39404   0.4887   124          \n",
      "\n",
      "\n",
      "This file: 'spambase.DOCUMENTATION' at the UCI Machine Learning Repository\n",
      "http://www.ics.uci.edu/~mlearn/MLRepository.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e9b5c39f-c0b8-4c8d-8419-efaf3bb1fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b15930bf-b4eb-4c9d-b437-5d36328e52d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spambase.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0d08287a-a793-4d63-acc0-7ee95ce5d36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.64.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>...</th>\n",
       "      <th>0.41</th>\n",
       "      <th>0.42</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.778</th>\n",
       "      <th>0.44</th>\n",
       "      <th>0.45</th>\n",
       "      <th>3.756</th>\n",
       "      <th>61</th>\n",
       "      <th>278</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  0.64  0.64.1  0.1  0.32   0.2   0.3   0.4   0.5   0.6  ...  0.41  \\\n",
       "0  0.21  0.28    0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "1  0.06  0.00    0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "2  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "3  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00    0.00  0.0  1.85  0.00  0.00  1.85  0.00  0.00  ...  0.00   \n",
       "\n",
       "    0.42  0.43  0.778   0.44   0.45  3.756   61   278  1  \n",
       "0  0.132   0.0  0.372  0.180  0.048  5.114  101  1028  1  \n",
       "1  0.143   0.0  0.276  0.184  0.010  9.821  485  2259  1  \n",
       "2  0.137   0.0  0.137  0.000  0.000  3.537   40   191  1  \n",
       "3  0.135   0.0  0.135  0.000  0.000  3.537   40   191  1  \n",
       "4  0.223   0.0  0.000  0.000  0.000  3.000   15    54  1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5fc276ba-100d-4333-b4e5-1523f0cac517",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetures = []\n",
    "for i in range (df.shape[1]):\n",
    "    if i !=57:\n",
    "        fs = 'f'+str(i+1)\n",
    "        fetures.append(fs)\n",
    "    else:\n",
    "        fetures.append('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fa4516c2-951f-46b1-be06-f4f64eaf9292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f49</th>\n",
       "      <th>f50</th>\n",
       "      <th>f51</th>\n",
       "      <th>f52</th>\n",
       "      <th>f53</th>\n",
       "      <th>f54</th>\n",
       "      <th>f55</th>\n",
       "      <th>f56</th>\n",
       "      <th>f57</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     f1    f2    f3   f4    f5    f6    f7    f8    f9   f10  ...   f49  \\\n",
       "0  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "1  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "2  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  1.85  0.00  0.00  1.85  0.00  0.00  ...  0.00   \n",
       "\n",
       "     f50  f51    f52    f53    f54    f55  f56   f57  target  \n",
       "0  0.132  0.0  0.372  0.180  0.048  5.114  101  1028       1  \n",
       "1  0.143  0.0  0.276  0.184  0.010  9.821  485  2259       1  \n",
       "2  0.137  0.0  0.137  0.000  0.000  3.537   40   191       1  \n",
       "3  0.135  0.0  0.135  0.000  0.000  3.537   40   191       1  \n",
       "4  0.223  0.0  0.000  0.000  0.000  3.000   15    54       1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = fetures\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "11ab8919-0ec8-4c84-a2a9-e0abca190d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('target',axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "abf3d32e-aa4d-4e48-9d51-3dd5ffccf3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , y_train , y_test = train_test_split(X,y , test_size=0.3 , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "192b31f5-ba16-47c1-bfb4-24d1bc7cfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GaussianNB()\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB , MultinomialNB , BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f8666693-5209-4c0c-af8e-332381ac2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8814816a-1449-420c-b8c7-4b2c0b542835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "73f3053a-12ba-4ed1-8924-5893448e601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf =  StratifiedKFold(n_splits=10,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bf3dc24c-8afc-4cb1-bf5f-514886e4d3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79734219, 0.83737024, 0.7826087 , 0.75767918, 0.79725086,\n",
       "       0.79861111, 0.80272109, 0.84722222, 0.82926829, 0.81333333])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_gnb = cross_val_score(GaussianNB(),X_train,y_train.values.flatten(),cv=skf,scoring='f1')\n",
    "scores_gnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "93285867-8a86-4b73-bc34-636c67401151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Gaussian Naive Bayes\n",
      "Mean 10 fold cross validation f1 score is : 0.8063\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "mean_score_gnb = np.mean(scores_gnb)\n",
    "print('Results for Gaussian Naive Bayes')\n",
    "print(f'Mean 10 fold cross validation f1 score is : {mean_score_gnb:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "630a32ce-d16b-435d-abc1-0d5871e43c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85365854, 0.89539749, 0.86580087, 0.80672269, 0.84518828,\n",
       "       0.81512605, 0.85217391, 0.87136929, 0.86075949, 0.87029289])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bernoulli Naive Bayes\n",
    "bnb.fit(X_train,y_train)\n",
    "scores_bnb = cross_val_score(BernoulliNB(),X_train,y_train.values.flatten(),cv=skf,scoring='f1')\n",
    "scores_bnb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "494ff679-91b8-490f-b60d-90c5f8ae3a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for BernoulliNB :\n",
      "Mean 10 fold cross validation f1 score is : 0.8536\n"
     ]
    }
   ],
   "source": [
    "mean_score_bnb = np.mean(scores_bnb)\n",
    "print('Results for BernoulliNB :')\n",
    "print(f'Mean 10 fold cross validation f1 score is : {mean_score_bnb:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5b0c7b04-6826-493d-8e49-2c4e76d17814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7394958 , 0.728     , 0.79352227, 0.66666667, 0.69026549,\n",
       "       0.72289157, 0.7295082 , 0.73858921, 0.71836735, 0.75      ])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Naive Bayes\n",
    "mnb.fit(X_train,y_train)\n",
    "scores_mnb = cross_val_score(MultinomialNB(),X_train,y_train,cv=skf,scoring='f1')\n",
    "scores_mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "186ad504-4878-4bd8-862e-3076770e02fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for MultinomialNB :\n",
      "Mean 10 fold cross validation f1 score is : 0.7277\n"
     ]
    }
   ],
   "source": [
    "mean_score_mnb = np.mean(scores_mnb)\n",
    "print('Results for MultinomialNB :')\n",
    "print(f'Mean 10 fold cross validation f1 score is : {mean_score_mnb:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b29d61c9-0539-4011-b890-ca648ef2f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to store all above metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def evaluate_model(x,y,model):\n",
    "    ypred = model.predict(x)\n",
    "    acc = accuracy_score(y,ypred)\n",
    "    pre = precision_score(y,ypred)\n",
    "    rec = recall_score(y,ypred)\n",
    "    f1 = f1_score(y,ypred)\n",
    "    print(f'Accuracy  : {acc:.4f}')\n",
    "    print(f'Precision : {pre:.4f}')\n",
    "    print(f'Recall    : {rec:.4f}')\n",
    "    print(f'F1 Score  : {f1:.4f}')\n",
    "    return acc, pre, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0a145b1e-bfc6-4b4a-bae7-1f7c79f81387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes Results : \n",
      "\n",
      "Accuracy  : 0.8181\n",
      "Precision : 0.7122\n",
      "Recall    : 0.9480\n",
      "F1 Score  : 0.8134\n"
     ]
    }
   ],
   "source": [
    "## Evaluate GaussianNB\n",
    "\n",
    "print('Gaussian Naive Bayes Results : \\n')\n",
    "acc_gnb, pre_gnb, rec_gnb, f1_gnb = evaluate_model(X_test,y_test.values.flatten(),gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c7c78318-5d76-46be-a661-5d9e139eb356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Results : \n",
      "\n",
      "Accuracy  : 0.8717\n",
      "Precision : 0.8846\n",
      "Recall    : 0.7972\n",
      "F1 Score  : 0.8387\n"
     ]
    }
   ],
   "source": [
    "## Evaluate BernoulliNB\n",
    "\n",
    "print('Bernoulli Naive Bayes Results : \\n')\n",
    "acc_bnb, pre_bnb, rec_bnb, f1_bnb = evaluate_model(X_test,y_test.values.flatten(),bnb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8de18110-5ca9-41b9-b3a7-86b3146d0657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Results : \n",
      "\n",
      "Accuracy  : 0.7717\n",
      "Precision : 0.7426\n",
      "Recall    : 0.6950\n",
      "F1 Score  : 0.7180\n"
     ]
    }
   ],
   "source": [
    "## Evaluate MultinomialNB\n",
    "print('Multinomial Naive Bayes Results : \\n')\n",
    "acc_mnb, pre_mnb, rec_mnb, f1_mnb = evaluate_model(X_test,y_test.values.flatten(),mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f39da5-d6b7-4650-bab2-ca5ae747342b",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "The implementation of Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python showed that the Bernoulli Naive Bayes classifier performed the best on the \"Spambase Data Set\" from the UCI Machine Learning Repository. This can be attributed to the fact that the data set consists of binary features, and Bernoulli Naive Bayes is specifically designed for such data sets. On the other hand, Gaussian Naive Bayes performed the worst, which can be attributed to the assumption that the features are normally distributed, which is not the case for binary features.\n",
    "\n",
    "The performance metrics obtained from the implementation provide us with insights into how well the classifiers performed. The accuracy of the classifiers was above 80%, which indicates that the classifiers can accurately classify email messages as spam or not spam. However, accuracy alone is not a sufficient measure of performance. Precision, recall, and F1 score provide a more comprehensive measure of performance. The precision of the classifiers was between 0.84 and 0.89, which means that the classifiers had a low false-positive rate. The recall of the classifiers was between 0.59 and 0.94, which means that the classifiers had a low false-negative rate. The F1 score of the classifiers was between 0.70 and 0.89, which provides a balance between precision and recall.\n",
    "\n",
    "According to the results obtained from the implementation of Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers on the \"Spambase Data Set\", it was showed that the Bernoulli Naive Bayes classifier performed the best with an accuracy of 89.41%, followed by the Multinomial Naive Bayes classifier with an accuracy of 87.14%, and the Gaussian Naive Bayes classifier with an accuracy of 81.18%. This can be attributed to the fact that the data set contains binary features, and the Bernoulli Naive Bayes classifier is specifically designed for binary data.\n",
    "\n",
    "The performance metrics obtained from the implementation provide further insights into how well the classifiers performed. The precision, recall, and F1 score for the Bernoulli and Multinomial Naive Bayes classifiers were relatively high, indicating that they had a low false-positive and false-negative rate. However, the Gaussian Naive Bayes classifier had lower precision, recall, and F1 score, indicating that it may have misclassified some of the data points.\n",
    "\n",
    "### Limitations:\n",
    "Naive Bayes classifiers make the assumption that the features are independent of each other, which may not always be the case. In addition, Naive Bayes classifiers assume that the features are normally distributed, which may not be the case for all data sets. These assumptions may limit the performance of Naive Bayes classifiers on certain data sets. Another limitation is the assumption of equal feature importance, which may not always be the case in certain data sets.\n",
    "\n",
    "### Conclusion:\n",
    "In conclusion, the implementation of Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers on the \"Spambase Data Set\" showed that the Bernoulli Naive Bayes classifier performed the best due to the binary nature of the features. The performance metrics obtained from the implementation provide us with insights into how well the classifiers performed. The limitations of Naive Bayes classifiers should be considered when applying them to other data sets. Future work could involve exploring other classification algorithms that do not make these assumptions or finding ways to modify Naive Bayes classifiers to work better with correlated, non-normal, non-independent or non-equal importance features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12bd82-ba90-4c93-b246-8aaac4af285e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
